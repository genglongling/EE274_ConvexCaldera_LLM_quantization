{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ITPj-IrCtuv6"
   },
   "outputs": [],
   "source": [
    "# Don't run this if you already have an environment set up with torch.\n",
    "\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "clYVKxgRuQkK"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "id": "yoMS5p40uYS4"
   },
   "outputs": [],
   "source": [
    "def data_aware_low_rank(\n",
    "    A: torch.Tensor,\n",
    "    X: torch.Tensor,\n",
    "    k: int,\n",
    "    tol: float = 1e-7\n",
    "    ):\n",
    "  \"\"\"\n",
    "  Compute the data-aware low-rank decomposition of a matrix by solving min_{L,R}||X(LR - A)||_F^2\n",
    "  A: Input matrix to be decomposed into low-rank factors\n",
    "  X: Calibration data matrix (each row is a datapoint)\n",
    "  k: Target rank\n",
    "  tol: For not inverting very small singular values\n",
    "  \"\"\"\n",
    "\n",
    "  m = X.shape[0]        # No. of datapoints\n",
    "  d = X.shape[1]        # Dimension of each datapoint\n",
    "\n",
    "\n",
    "  assert A.shape[0] == d, \"Dimension mismatch between X and A!\"\n",
    "  assert m >= k, \"The number of datapoints should be larger than the target rank.\"\n",
    "\n",
    "  n = A.shape[1]\n",
    "\n",
    "  Y = X @ A\n",
    "  U, Sigmat, Vh = torch.linalg.svd(X, full_matrices=True)\n",
    "  V = Vh.T\n",
    "\n",
    "  if m <= d:\n",
    "\n",
    "    Ub, Sigmab, Vbh = torch.linalg.svd(U.T @ Y, full_matrices=True)\n",
    "\n",
    "    print(f\"Sigmab.shape: {Sigmab.shape}\")\n",
    "    print(f\"Vbh.shape: {Vbh.shape}\")\n",
    "\n",
    "    inv_sing_vals = torch.where(Sigmat >= tol, 1.0 / Sigmat, torch.tensor(0.0))       # Pseudo-inverse\n",
    "    Sigma_inv = torch.diag(inv_sing_vals)\n",
    "\n",
    "    L = V[:,:m] @ Sigma_inv @ Ub[:,:k]\n",
    "\n",
    "    pad = torch.zeros((m, n-m))\n",
    "    Sigmab = torch.cat((torch.diag(Sigmab), pad), dim=1)\n",
    "    print(f\"Sigmab.shape = {Sigmab.shape}\")\n",
    "    R = Sigmab[:k,:] @ Vbh\n",
    "\n",
    "  else:\n",
    "\n",
    "    Ub, Sigmab, Vbh = torch.linalg.svd(U[:,:d].T @ Y, full_matrices=True)\n",
    "\n",
    "    inv_sing_vals = torch.where(Sigmat >= tol, 1.0 / Sigmat, torch.tensor(0.0))       # Pseudo-inverse\n",
    "    Sigma_inv = torch.diag(inv_sing_vals)\n",
    "\n",
    "    L = V @ Sigma_inv @ Ub[:, :k]\n",
    "    R = torch.diag(Sigmab)[:k,:] @ Vbh\n",
    "\n",
    "  return {\"L\": L, \"R\": R}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "j1iBQP1aa7g2"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "def data_aware_low_rank_regH(\n",
    "    A: torch.Tensor,\n",
    "    H: torch.Tensor,\n",
    "    k: int,\n",
    "    sigma_reg: float = 1e-5\n",
    "    ):\n",
    "  \"\"\"\n",
    "  Compute the data-aware low-rank decomposition with regularized Hessian by solving min_{L,R}||(A - LR)H^{1/2}||_F^2\n",
    "  A: Input matrix to be decomposed into low-rank factors\n",
    "  H: Input Hessian H = X'X\n",
    "  k: Target rank\n",
    "  sigma_reg: Regularization tolerance for not inverting very small eigenvalues\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  assert H.shape[0] == H.shape[1] and torch.allclose(H, H.T), \"Hessian is not symmetric.\"\n",
    "  assert A.shape[1] == H.shape[0], \"Dimension mismatch between A and H.\"\n",
    "\n",
    "  # Named tuple to save eigenvalues and eigenvectors\n",
    "  EigTuple = namedtuple(\"EigTuple\", [\"eigenvalues\", \"eigenvectors\"])\n",
    "\n",
    "  # Compute the eigenvalue decomposition of the Hessian and regularize to make it positive definite\n",
    "  eigH = torch.linalg.eigh(H)\n",
    "  eigvals = eigH.eigenvalues\n",
    "  if eigvals.min() < sigma_reg:\n",
    "      H = H + (sigma_reg - eigvals.min()) * torch.eye(H.shape[0])\n",
    "      eigvals += sigma_reg - eigvals.min()\n",
    "      eigH = EigTuple(eigvals, eigH.eigenvectors)\n",
    "\n",
    "  # Symmetric square root of Hessian\n",
    "  H_sqrt = (eigH.eigenvectors @ torch.diag(torch.sqrt(eigvals)) @ eigH.eigenvectors.T)\n",
    "\n",
    "  # Compute low-rank factors\n",
    "  Y = A @ H_sqrt @ eigH.eigenvectors\n",
    "  U, Sigma, Vh = torch.linalg.svd(Y, full_matrices=False)\n",
    "\n",
    "  L = U[:, :k]\n",
    "  R = torch.diag(Sigma[:k]) @ Vh[:k, :] @ torch.diag(1 / eigH.eigenvalues.sqrt()) @ eigH.eigenvectors.T\n",
    "\n",
    "  return {\"L\": L, \"R\": R}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "GFv55yoJGSmA"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "d = 8\n",
    "n = 8\n",
    "m = 10\n",
    "k = 5\n",
    "\n",
    "A = torch.randn(d,n)\n",
    "X = torch.randn(m,d)\n",
    "H = X.T @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0mulLUOHHHx"
   },
   "outputs": [],
   "source": [
    "result = data_aware_low_rank_regH(A, H, k)\n",
    "\n",
    "L = result[\"L\"]\n",
    "R = result[\"R\"]\n",
    "\n",
    "print(f\"H.shape: {H.shape}\")\n",
    "print(f\"A.shape: {A.shape}\")\n",
    "\n",
    "print(f\"L.shape: {L.shape}\")\n",
    "print(f\"R.shape: {R.shape}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
